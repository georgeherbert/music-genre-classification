\documentclass[conference]{IEEEtran}
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[font=small]{caption}
\usepackage{subcaption}
% \usepackage{textcomp}
% \usepackage{xcolor}
\graphicspath{{./images/}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\captionsetup[table]{
    justification=centerlast,
    labelsep=newline,
    textfont={footnotesize, sc},
    labelfont=footnotesize,
    skip=0pt,
}

\title{Shallow Convolutional Neural Network Architectures for Music Genre Classification}

\author{\IEEEauthorblockN{George Herbert}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University of Bristol}\\
Bristol, United Kingdom \\
cj19328@bristol.ac.uk}
}

\maketitle

\begin{abstract}
In this paper I implement and evaluate the shallow convolutional neural network architecture proposed by Schindler et al. \cite{SchindlerLidyRauber} for the task of music genre categorisation.
In the first part of this paper, I describe the process I undertook to implement the network.
While in the latter part, I evaluate the results my implementation achieved, and extend on their work by implementing batch normalisation.
\end{abstract}

\begin{IEEEkeywords}
Music Information Retrieval, Music Genre Classification, Convolutional Neural Networks
\end{IEEEkeywords}

\section{Introduction}

Explosive growth in the audio streaming industry has fuelled rapid developments in the interdisciplinary science of music information retrieval (MIR).
Applications of MIR are of evergrowing importance to the music industry to improve tools such as recommender systems, which can offer a competitive advantage.
Genre classification is one of the most fundamental problems in MIR.
Early approaches, such as that by Tzanetakis and Cook \cite{TzanetakisCook}, focused on training statistical classifiers using vector summaries of features such as timbral texture, rhythmic content and pitch content; however, these summaries are absent of the important temporal structure in the underlying audio.
More recently, audio spectrograms---which represent frequency data over time---have been widely investigated and used to train state-of-the-art deep-learning architectures.

To this respect, this paper explores the shallow convolutional neural network (CNN) architecture proposed by Schindler et al. \cite{SchindlerLidyRauber}.
CNNs have been widely investigated in the context of genre classification following their successes in the field of computer vision.

\section{Related Work}

Liu et al. \cite{LiuFengLiuWangLiu} recently proposed a novel architecture named a Bottom-up Broadcast Neural Network (BBNN) to deal with some of the problems traditionally associated with genre classification.
They identified that many previously developed architectures had focused on abstracting high-level semantic features layer-by-layer; as a result, these architectures suffer from a huge loss of lower-level features which are critical to the task of genre classification.
Thus, the BBNN architecture was specifically designed to simultaneously abstract high-level information while preserving the lower-level features.

Large datasets are frequently required to train powerful deep neural networks.
However, in the MIR domain, there is often  a lack of large training datasets.
Hung et al. \cite{HungYangChenLerch} published some very recent work that successfully dealt with this problem.
They introduced a novel method called input-dependent neural model reprogramming---a transfer learning training scheme---that leverages pre-trained models for music classification.
They successfully applied this method to reprogram two published models that were pre-trained on speech and audio data.

\section{Dataset}

I used the GTZAN dataset, compiled by Tzanetakis and Cook \cite{TzanetakisCook}, to train and evaluate my models.
The dataset contains a total of 1000 WAV audio tracks, each 30 seconds in length.
GTZAN is a balanced dataset, containing 100 tracks for each of the 10 genres in the dataset: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae and rock.

I utilised a stratified-by-genre split to produce a training and test set: the training set contained 75\% of the audio tracks, 75 tracks from each genre; while the test set contained the other 25\% of audio tracks.

\section{CNN Architecture (Schindler et al.)}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{architecture.jpg}}
    \caption{Abstract representation of the shallow CNN architecture as described by Schindler et al.}
    \label{architecture}
\end{figure}

I recreated the shallow CNN architecture outlined by Schindler et al. \cite{SchindlerLidyRauber}; Figure \ref{architecture} displays an abstract representation of the architecture.
Log-mel spectrograms of shape $80\times80$ are provided as input to the network.
Since the dimensions of the spectrograms correspond to time and frequency, the network utilises a parallel architecture.

The top pipeline aims to capture frequency relations.
It first contains a convolutional layer (with padding) with 16 kernels of shape $10\times23$ to produce 16 square feature maps of shape $80\times80$.
These are then downsampled using a $1\times20$ max pooling layer to produce 16 vertical rectangular feature maps of shape $80\times4$.
The kernel and max pooling shapes were specifically selected to capture spectral characteristics.
Conversely, the bottom pipeline aims to capture temporal relations.
It too initially contains a convolutional layer (with padding) with 16 kernels, but of approximately square shape $21\times20$ to produce 16 square feature maps of shape $80\times80$.
These too are then downsampled using a $20\times1$ max pooling layer to produce 16 horizontal rectangular feature maps of shape $4\times80$, specifically to capture temporal changes in intensity.

The 16 feature maps from each piepline are flattened and concatenated to a shape of $1\times10240$, which is mapped to a 200 neuron fully-connected layer.
These final 200 neurons are then mapped to 10 output neurons---10\% dropout is utilised at this layer to mitigate overfitting.
The softmax function is applied to these 10 output neurons to produce a pseudo-probability distribution to represent the probability that a given input belongs to each of the 10 genres.

With the exception of the final layer, each convolutional and fully-connected layer is passed through the Leaky ReLU activation function.

\section{Implementation Details}

\subsection{Preprocessing}

The network was trained and evaluated using a total of 15000 log-mel spectrograms.
Each audio track in the GTZAN dataset was split into chunks of approximately 0.93 seconds, using a step size of 50\%.
Log-mel spectrograms of shape $80\times80$ were produced from 15 randomly selected chunks from each track using a fast Fourier transform with a window size of 1024, and a step size of 50\%.
It was imperative to split the tracks into the training and test sets prior to producing the spectrograms to avoid data leakage.

\subsection{Training Details}

I constructed and trained the CNN using Python and the PyTorch \cite{PyTorch} machine learning framework.
I reimplemented the training process as described by Schindler et al. \cite{SchindlerLidyRauber}.
To this respect, I used cross-entropy loss to evaluate the performance of the network, with L1 regularisation to mitigate overfitting using a penalty value of $0.0001$.
I used the Adam optimiser \cite{KingmaBa}---an extension of stochastic gradient descent---to optimise the network, using a learning rate of $5\times10^{-5}$, $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=1\times10^{-8}$ for numerical stability.
I trained the network on a BlueCrystal Phase 4 GPU node, which contains two NVIDIA Tesla P100 GPUs \cite{bc4}.

\subsection{Weight initialisation}

Weight initialisation is an important design choice, since it determines the starting point of the optimisation procedure.
Schindler et al. did not specify the procedure they used.
However, modern weight initialisation heuristics exist that depend on the activation function.
In this respect, I implemented He initialisation using due to the Leaky ReLU activation function being used throughout the network.

\subsection{Batch size}

Batch size is a fundamental hyperparameter to consider when training deep networks.
Smaller batches give rise to longer epochs and introduce extra noise to the weight updates; however, this noise can prove beneficial if the error manifold has many deep local optima.
Conversely, larger batches give rise to shorter epochs, but networks trained with large batches often struggle to generalise.
Schindler et al. did not identify the batch size they used for training.
Thus, I experimented with multiple batch sizes, and found that my results were most similar with a batch size of 64.

\section{Replicating Quantitative Results}

Table \ref{shallow_results} displays the mean accuracy my implementation achieved on the test set over five runs; the accuracy achieved by Schindler et al. \cite{SchindlerLidyRauber} is also displayed for comparison.
I trained my network for a total of 500 epochs, but there was negligable difference in accuracy after 200 epochs because the network had already overfit---achieving a test accuracy of near 100\%---so training the network for additional epochs did not improve the network's ability to generalise.
In this respect, if the network were trained for longer, it is likely the test accuracy would decrease.

\begin{table}[htbp]
    \caption{Accuracy achieved on the test set}
    \begin{center}
    \begin{tabular}{l c c}
    \toprule
    \textbf{Model}&\textbf{Epoch}&\textbf{Accuracy}\\
    \midrule
    \multirow{ 2}{*}{My CNN} & 100 & 62.96 \\
    & 200 & 64.01 \\
    \midrule
    \multirow{ 2}{*}{Schindler et al.} & 100 & 66.56\\
    & 200 & 67.49 \\
    \bottomrule
    \end{tabular}
    \label{shallow_results}
    \end{center}
\end{table}

Notably, the accuracy achieved is approximately 3\% lower than that of Schindler et al.
This discrepency can potentially be attributed to Schindler et al. not specifying the full design details necessary for reproducibility.
Importantly, Schindler et al. produced their implementation in Keras, which often has different default parameters than Pytorch.

Figure \ref{confusion_matrix} is a confusion matrix displaying the the performance of my implementation after 200 epochs.
Notably, there is a large inconsistency in the per-class accuracy of different genres.
For example, the network achieved a high per class accuracy of over 80\% on the blues, classical and metal genres.
While conversely, it achieved less than a 40\% per-class accuracy on the reggae and rock genres; in particular, it misclassified 21\% of rock songs as blues.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{cm.png}}
    \caption{
        Confusion matrix displaying the performance of my network on the test set after 200 epochs for a single training run.    
        The value in a given cell represents the proportion of samples from the true genre categorised as the predicted genre.
    }
    \label{confusion_matrix}
\end{figure}

\section{Training Curves}

Overfitting occurs when a network learns the noise in the data as if it represents the structure of the underlying model.
To detect overfitting, I monitored the loss and accuracy my network achieved on both the training and test set throughout the training process.
Figure \ref{accuracy_curves} and Figure \ref{loss_curves} display the accuracy and loss curves for my network, respectively.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{accuracy.png}}
    \caption{
        Plot of accuracy data from the same training run as Figure \ref{confusion_matrix}.
        The line labelled `training' is the accuracy achieved on the training set, calculated every 100 batches; the line labelled `test' is the accuracy achieved on the test set, calculated every epoch.}
    \label{accuracy_curves}
\end{figure}

After 200 epochs, there was a discrepency of approximately 35\% in the accuracy of the model on the training and test sets---this strongly indicates that the shallow CNN architecture overfit.
Despite using L1 regularisation and dropout, the model had memorised almost every sample in the training set, and therefore struggled to generalise to unseen data.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{loss.png}}
    \caption{
        Plot of loss data from the same training run as Figure \ref{confusion_matrix}.
        The line labelled `training' is the loss achieved on the training set, calculated every 100 batches; the line labelled `test' is the loss achieved on the test set, calculated every epoch.
    }
    \label{loss_curves}
\end{figure}

\section{Qualitative Results}

Deep neural networks are frequently described as black-box models, because it can be exceptionally difficult to reason about the features they extract to produce their outputs.
% I analysed log-mel spectrograms classified by my network, as well as their respective audio files, to reason about the areas in which it succeeds and fails.
Figure \ref{spectrograms} displays three log-mel spectrograms produced from samples from the audio tracks in the GTZAN dataset.
An analysis of these spectrograms, as well as listening to the audio files from which they were derived, yields some clues as to the discrepencies in per-class accuracy that the network achieved.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth}
        \centerline{\includegraphics[width=\columnwidth]{spec_correct.png}}
        \caption{classical.00070}
        \label{spec_correct}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centerline{\includegraphics[width=\columnwidth]{spec_incorrect_1.png}}
        \caption{jazz.00003}
        \label{spec_incorrect_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centerline{\includegraphics[width=\columnwidth]{spec_incorrect_2.png}}
        \caption{Correct}
        \label{spec_incorrect_2}
    \end{subfigure}
    \caption{Log-mel spectrograms produced from three of the samples in the test dataset.}
    \label{spectrograms}
\end{figure}

The spectrogram in Figure \ref{spec_correct} was correctly classified as belonging to the classical genre, and as were the other 14 log-mel spectrograms extracted from `classical.00070.wav'.
The audio within the file is very characteristic of classical music, with long notes played from a violin.
Harmonics produced by the violin are represented by the long parallel lines that stretch across the entirety of the file.
An analysis of other spectrograms produced from classical music appear very similar.
One can see how a neural network could relatively easily learn that extended parallel lines such as these indicate classical music.

Conversely, the network incorrectly predicetd the class for all spectrograms produced from `jazz.00003.wav'.
A spectrogram produced from a segment of the audio file is shown in Figure \ref{spec_incorrect_1}, in which there are long parallel lines characteristic of classical music.
In fact, nine of the spectrograms derived from the file were classified as being classical.
This lends credence to the idea that the network classifies spectrograms with long parallel lines as being classical music.

The spectrogram in Figure `pop.00064.wav' was also incorrectly classifed, as were 13 of the other spectrograms derived from the same file.
In fact, 11 of the spectrograms were incorrectly classified as being disco music.
This arguably highlights a fundamental problem with using genre as a way of classifying music: its subjective nature.
Genres are often not defined objectively.
It is highly unlikely even a human would agree with all of the genre classiciations in the dataset, which makes it understandable that a machine too would incorrectly classify so many samples.

\section{Improvements}

\subsection{Maximum Probability and Majority Voting}

Thus far, I have computed the `raw' accuracy of my model, by predicting the genre for each sample in my dataset independently.
However, this is not the optimal method since it ignores all file dependencies---there are 15 spectrograms for each WAV audio track in the GTZAN dataset.
Schindler et al. recognised this, and achieved a substantial improvement in accuracy by introducing two new methods of classification: maximum probability and majority vote, both of which take file dependencies into account.
To classify an inputted audio file by maximum probability, the probabilities output by the final softmax layer for each segment are first summed; then, the predicted class is determined by the largest value amongst the summed probabilities.
To classify an inputted audio file by majority vote, a class is determined for each segment by the largest value output by the final layer; then, a majority vote is conducted over the predicted classes for each of the segments.

Table \ref{improved_results} displays the accuracy achieved using each of the raw, maximum probability and majority vote approaches.
The relative improvement in accuracy provided by the two new approaches are consistent with the findings of Schindler et al.
The increase in accuracy is due to to there being some level of independence in the predictions of samples from a given file.
This independence can be attributed to the 0.93 second samples often being too short to be representative, as well as the model overfitting the training data.
The majority vote and maximum probability classification methods essentially cancel out many of the individual errors to produce more accurate predictions for entire files.

\begin{table}[htbp]
    \caption{Improved accuracy achieved on the test set}
    \begin{center}
    \begin{tabular}{l c c c c}
    \toprule
    &&\multicolumn{3}{c}{\textbf{Accuracy}}\\
    \cmidrule(lr){3-5}
    \textbf{Model}&\textbf{Epoch}&\textbf{Raw}&\textbf{Max}&\textbf{Maj}\\
    \midrule
    \multirow{ 2}{*}{My CNN} & 100 & 62.96 & 75.80 & 73.53 \\
    & 200 & 64.01 & 77.60 & 76.60 \\
    \midrule
    \multirow{ 2}{*}{My CNN + Batch Norm} & 100 & 66.30 & 77.30 & 77.56 \\
    & 200 & \textbf{66.55} & \textbf{78.68} & \textbf{77.64} \\
    \bottomrule
    \end{tabular}
    \label{improved_results}
    \end{center}
\end{table}

\subsection{Batch Normalisation}

Batch normalisation \cite{IoffeSzegedy} is a technique frequently employed to increase the speed and stability of the training process.
This is achieved via a normalisation step that fixes the means and variances of each layer's input.
When applied to the output from a convolutional layer, this is implemented as follows:
\[
\hat{x}_{i,c,x,y}=\gamma_c\frac{x_{i,c,x,y}-\mu_c}{\sqrt{\sigma_c^2+\epsilon}}+\beta_c
\]
where $x$ is the output from the previous layer; $\hat{x}$ is the input to the subsequent layer; $i$, $c$, $x$ and $y$ are the batch, channel, width and height indices; $\gamma_c$ and $\beta_c$ are parameters learned during the optimisation process to restore the representative power of the network; and $\gamma$ is a small constant for numerical stability.

I implemented a two-dimensional batch normalisation layer following each of the two convolutional layers in my network using $PyTorch$'s \texttt{nn.BatchNorm2d} class.
Table \ref{improved_results} displays the accuracy of my network with the inclusion of the batch normalisation layers.
Not only did batch normalisation provide a reasonabe improvement in accuracy after 200 epochs---especially in the case of raw accuracy---but, it also reduced the number of epochs required for convergence to a small degree.
Unlike in the case of the CNN trained without batch normalisation, the difference in accuracy between 100 and 200 epochs is very minimal.
The precise reasons for the improved performance are unclear: Batch normalisation has strong empirical performance, but there is some disagreement as to the theory behind its effectiveness.
Santukar et al. \cite{SanturkarEtAl} proposed that batch normalisation significantly smoothes the optimisation landscape, which induces a more predictive and stable behaviour of the gradients, thus facilitating faster training.

Figure \ref{accuracy_curves_improved} displays the raw accuracy the network achieved on the training and test set during the training process.
It is evident that the degree to which the network is overfitting has been reduced.
This is because batch normalisation also works as a regulariser, due to the fact that the mean and standard deviation is computed from each mini-batch, rather than the entire dataset.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{accuracy_improved.png}}
    \caption{
        Plot of accuracy data for my improved network from a single training run.
        Format is the same as for Figure \ref{accuracy_curves}.
    }
    \label{accuracy_curves_improved}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{cm_improved.png}}
    \caption{
        Confusion matrix displaying the performance of my improved network with batch normalisation on the test set after 200 epochs for a single training run.
        Classification was performed using the maximum probability method.
        Format is the same as for Figure \ref{confusion_matrix}.
    }
    \label{confusion_matrix_improved}
\end{figure}

\section{Conclusion and Future Work}

In this paper I have reimplemented the shallow CNN architecture as originally described by Schindler et al. \cite{SchindlerLidyRauber}.
While I was unable to reproduce the results published in the original paper, I have exposed a significant discrepency in the per-class accuracy when the the network is trained using the GTZAN dataset.
Moreso, I have conducted an extensive qualitative analysis to gain an insight into potential reasons for this discrepency.

In the latter part of this paper, I have also proposed an extension to the architecture with the inclusion of two batch normalisation layers.
I found that the inclusion of these layers offered an advantage over the original architecture by reducing the time required for convergence, as well as providing a modest improvement to accuracy.

Future work should focus on eliminating the stark discrepency in per-class accuracy.
Further extensions should also be considered with the aim of reducing the degree to which the network overfits.
Data augmentation provides a potential avenue to explore in this respect.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
