\documentclass[conference]{IEEEtran}
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{textcomp}
% \usepackage{xcolor}
\graphicspath{{./images/}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Shallow Convolutional Neural Network Architectures for Music Genre Classification}

\author{\IEEEauthorblockN{George Herbert}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University of Bristol}\\
Bristol, United Kingdom \\
cj19328@bristol.ac.uk}
}

\maketitle

\begin{abstract}
In this paper I investigate the shallow convolutional neural network architecture proposed by Schindler et al. \cite{SchindlerLidyRauber} for the task of music genre categorisation.
In the first part of this paper, I attempt to replicate the results achieved by Schindler et al., while in the latter part, I extend on their work by 
\end{abstract}

\begin{IEEEkeywords}
music information retrieval, convolutional neural networks
\end{IEEEkeywords}

\section{Introduction}

Music genre classification---categorising a music sample into one or more genres---is a fundamental problem in music information retrieval.
Early approaches to genre classification, such as that by Tzanetakis and Cook \cite{TzanetakisCook}, focused on training statistical classifiers using features such as timbral texture, rhythmic content and pitch content.
More recently, convolutional neural networks (CNNs) have been widely investigated in the context of genre classification, following their successes in the field of computer vision.

In one such paper, Schindler et al. \cite{SchindlerLidyRauber} investigated the performance of two different CNN architectures; in conjunction, they examined how data augmentation applied to four different datasets impacted the performance of both architectures.

\section{Related Work}

\section{Dataset}

I used the the GTZAN dataset compiled by Tzanetakis and Cook \cite{TzanetakisCook} to train and validate my models; it contains 1000 WAV audio tracks, each 30 seconds in length.
There are 100 tracks for each of the 10 genres in the dataset: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae and rock.
The CNNs were trained on chunks of approximately 0.93 seconds that were randomly selected from each audio track.

To produce a training and validation set, a stratified split was deemed suitable to prevent imbalance.
Chunks from 75\% of the WAV audio tracks for each genre were randomly selected to make up the training set, with chunks from the other 25\% of audio tracks for each genre making up the validation set.

\section{CNN Architecture (Schindler et al.)}

I recreated the shallow CNN architecture outlined by Schindler et al.
Log-mel spectrograms of shape $80\times80$ are provided as input to the network.

Since the dimensions of the spectrograms correspond to time and frequency, Schindler et al. implemented a parallel architecture.
The left pipeline aims to capture frequency relations.
It first contains a convolutional layer (with padding) with 16 kernels of shape $10\times23$ to produce 16 square feature maps of shape $80\times80$.
These are then downsampled using a $1\times20$ max pooling layer to produce 16 vertical rectangular feature maps of shape $80\times4$.
The kernel and max pooling shapes were specifically selected to capture spectral characteristics.
Conversely, the right pipeline aims to capture temporal relations.
It too initially contains a convolutional layer (with padding) with 16 kernels, but of approximately square shape $21\times20$ to produce 16 square feature maps of shape $80\times80$.
These are then downsampled using a $20\times1$ max pooling layer to produce 16 horizontal rectangular feature maps of shape $4\times80$, specifically to capture temporal changes in intensity.

The 16 feature maps from each piepline are flattened and concatenated to a shape of $1\times10240$, which serves as input to a 200 neuron fully connected layer---10\% dropout is utilised at this layer to prevent overfitting.
These final 200 neurons are then mapped to 10 output neurons, which represent the probabilities of each of the ten genres for a given input.

With the exception of the final layer, each convolutional and fully connected layer is passed through the Leaky ReLU activation function.
The final layer uses the softmax activation function.

\section{Implementation Details}

I used the \textit{PyTorch} \cite{PyTorch} machine learning framework to implement and train my models.

\subsection{Optimiser}

I optimised my network using the Adam optimisation algorithm \cite{KingmaBa}, as implemented by the \texttt{torch.optim.Adam} class, with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1\times10^{-8}$ and a learning rate of $5\times10^{-5}$.

\subsection{Loss function}

I implemented the cross-entropy loss function using $PyTorch$'s \texttt{torch.nn.CrossEntropyLoss} class to measure the error between the output of the network and the label encoded in a one-hot representation.

\subsection{Batch size}

Schindler et al. did not specify the batch size they used for training.

\subsection{Weight initialisation}

Schindler et al. did not specify how they initialised the weights in their network.

\subsection{\texttt{ShallowCNN}}

To implement the CNN I created a \texttt{ShallowCNN} class that inherit's from the \texttt{torch.nn.Modules} class.
I created a \texttt{forward} method in the class.

\subsection{\texttt{Trainer}}

I created a \texttt{Trainer} class 

BlueCrystal Phase 4

\section{Replicating Quantitative Results}

Table \ref{shallow_results} displays the accuracy my implementation achieved on the validation dataset at both 100 and 200 epochs.

\begin{table}[htbp]
    \centering
    \caption{Shallow CNN Accuracy on the Validation Set}
    \label{shallow_results}
    \begin{IEEEeqnarraybox}[\IEEEeqnarraystrutmode\IEEEeqnarraystrutsizeadd{2pt}{1pt}]{v/c/v/c/v}
    \IEEEeqnarrayrulerow\\
    &\mbox{Epoch}&&\mbox{Accuracy (\%)}&\\
    \IEEEeqnarraydblrulerow\\
    \IEEEeqnarrayseprow[3pt]\\
    &100&&63.71&\IEEEeqnarraystrutsize{0pt}{0pt}\\ \IEEEeqnarrayseprow[3pt]\\
    \IEEEeqnarrayrulerow\\
    \IEEEeqnarrayseprow[3pt]\\
    &200&&65.09&\IEEEeqnarraystrutsize{0pt}{0pt}\\
    \IEEEeqnarrayseprow[3pt]\\
    \IEEEeqnarrayrulerow
    \end{IEEEeqnarraybox}
\end{table}

\section{Training Curves}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{accuracy.png}}
    \caption{Accuracy curves}
    \label{accuracy_curves}
\end{figure}

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\columnwidth]{loss.png}}
    \caption{Loss curves}
    \label{loss_curves}
\end{figure}

\section{Qualitative Results}

\section{Improvements}

\section{Conclusion and Future Work}



\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
